{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CodeSearch, this notebook simply attempts to reproduce the results in the paper [\"CodeBERT:\n",
    "A Pre-Trained Model for Programming and Natural Languages\"](https://arxiv.org/pdf/2002.08155.pdf). It very closely follows its framework as well, but here we only focus on the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from io import open\n",
    "from itertools import cycle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, TensorDataset\n",
    "from transformers import (AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base', num_labels=2, finetuning_task=\"codesearch\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base', do_lower_case=True)\n",
    "model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTextData:\n",
    "    \n",
    "    def __init__(self, text0, text1, label):\n",
    "        self.text0 = text0\n",
    "        self.text1 = text1\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedData:\n",
    "    \n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_text(dir_filename, encoding='utf-8'):\n",
    "    with open(dir_filename, \"r\", encoding='utf-8') as f:\n",
    "        raw_text = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split('<CODESPLIT>')\n",
    "            if len(line) != 5:\n",
    "                continue\n",
    "            raw_text.append(RawTextData(text0=line[3], text1=line[4], label=line[0]))\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_text = get_raw_text(\"python_train_val/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_raw_text(text_data, tokenizer, max_seq_length=50):\n",
    "    pad_token=0\n",
    "    pad_token_segment_id=0\n",
    "    features= []\n",
    "    print(\"*** Tokenizing Data *** \")\n",
    "    for i, example in enumerate(text_data):\n",
    "        if i % 1600 == 1599:\n",
    "            print(\"=\", end=\"\")\n",
    "        if i % 64000 == 63999:\n",
    "            print(\"[Processed \" + str(i+1) + \" / \" + str(len(text_data)) + \"] \" )    \n",
    "        tokens0 = tokenizer.tokenize(example.text0)[:max_seq_length]\n",
    "        tokens1 = tokenizer.tokenize(example.text1)\n",
    "        # Truncates the sequence so that its length is at most max_seq_length - 3\n",
    "        # This takes into account the [SEP] and [CLS] tokens required by BERT\n",
    "        while len(tokens0) + len(tokens1) > max_seq_length - 3:\n",
    "            if len(tokens0) > len(tokens1):\n",
    "                tokens0.pop()\n",
    "            else:\n",
    "                tokens1.pop()\n",
    "        tokens = tokens0 + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        \n",
    "        tokens += tokens1 + [\"[SEP]\"]\n",
    "        segment_ids += [1] * (len(tokens1) + 1)\n",
    "        tokens = [\"[CLS]\"] + tokens\n",
    "        segment_ids = [1] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # If necessary, zero-pad \n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        input_mask = input_mask + ([0] * padding_length)\n",
    "        segment_ids = segment_ids + ([0] * padding_length)\n",
    "        \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            TokenizedData(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label=int(example.label)))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Tokenizing Data *** \n",
      "========================================[Processed 64000 / 824342] \n",
      "========================================[Processed 128000 / 824342] \n",
      "========================================[Processed 192000 / 824342] \n",
      "========================================[Processed 256000 / 824342] \n",
      "========================================[Processed 320000 / 824342] \n",
      "========================================[Processed 384000 / 824342] \n",
      "========================================[Processed 448000 / 824342] \n",
      "========================================[Processed 512000 / 824342] \n",
      "========================================[Processed 576000 / 824342] \n",
      "========================================[Processed 640000 / 824342] \n",
      "========================================[Processed 704000 / 824342] \n",
      "========================================[Processed 768000 / 824342] \n",
      "==================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "824342"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = tokenize_raw_text(raw_text, tokenizer)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "train_input_masks = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "train_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "train_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "train_dataset = TensorDataset(train_input_ids, train_input_masks, train_segment_ids, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "adam_epsilon = 1e-8\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import optim\n",
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, model, optimizer, batch_size=100, num_epochs=8):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "\n",
    "    t_total = len(train_dataloader) // num_epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, t_total)\n",
    "    \n",
    "    print(\"*** Training ***\")\n",
    "\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    for idx, _ in enumerate(range(num_epochs)):\n",
    "        cumu_loss, curr_loss = 0.0, 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                     'labels': batch[3]}\n",
    "            outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[3])\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            cumu_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "            if step % 50 == 49:\n",
    "                print(\"=\", end=\"\")\n",
    "            if step % 800 == 799:\n",
    "                print('[Epoch %d / %d, minibatch %d / %d] loss: %.5f' %\n",
    "                  (idx + 1, num_epochs, step + 1, len(train_dataloader), (cumu_loss - curr_loss) / 800))\n",
    "                curr_loss = cumu_loss\n",
    "    return global_step, cumu_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step, training_loss = train(dataset, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'roberta-model__v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('roberta-model__v2.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('python_test/batch_0.txt', \"r\", encoding='utf-8') as f:\n",
    "    test0_lines = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split('<CODESPLIT>')\n",
    "        if len(line) != 5:\n",
    "            continue\n",
    "        test0_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0_examples = []\n",
    "for (i, line) in enumerate(test0_lines):\n",
    "    guid = \"%s-%s\" % ('test', i)\n",
    "    text_a = line[3]\n",
    "    text_b = line[4]\n",
    "    label = \"0\"\n",
    "    test0_examples.append(\n",
    "        InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0_features = convert_examples_to_features(test0_examples,\n",
    "                                       ['0', '1'],\n",
    "                                       max_seq_length=50,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       output_mode='classification'\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0_input_ids = torch.tensor([f.input_ids for f in test0_features], dtype=torch.long)\n",
    "test0_input_mask = torch.tensor([f.input_mask for f in test0_features], dtype=torch.long)\n",
    "test0_segment_ids = torch.tensor([f.segment_ids for f in test0_features], dtype=torch.long)\n",
    "test0_label_ids = torch.tensor([f.label_id for f in test0_features], dtype=torch.long)\n",
    "test0_dataset = TensorDataset(test0_input_ids, test0_input_mask, test0_segment_ids, test0_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, dataset, lines, output_test_file, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluates the model based on classification accuracy. Receives the logits that are output\n",
    "    from the network and saves the result in the given output directory file.\n",
    "    \"\"\"\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=batch_size)\n",
    "\n",
    "    print(\"*** Evaluating ***\")\n",
    "    eval_loss = 0.0\n",
    "    num_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for i, batch in enumerate(eval_dataloader):\n",
    "        if i % 200 == 199:\n",
    "            print(\"=\", end=\"\")\n",
    "        if i % 5000 == 4999:\n",
    "            print(\"[Step \" + str(i+1) + \" / \" + str(len(eval_dataloader)) + \"] \" )\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            labels = batch[3]\n",
    "            outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            \n",
    "        num_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = labels.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "            \n",
    "    eval_loss = eval_loss / num_steps\n",
    "    \n",
    "    preds_label = np.argmax(preds, axis=1)\n",
    "    \n",
    "    accuracy = (preds_label == out_label_ids).mean()\n",
    "    output_dir = os.path.dirname(output_test_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    with open(output_test_file, \"w\") as writer:\n",
    "        all_logits = preds.tolist()\n",
    "        for i, logit in enumerate(all_logits):\n",
    "            line = '<CODESPLIT>'.join(\n",
    "                [item.encode('ascii', 'ignore').decode('ascii') for item in lines[i]])\n",
    "\n",
    "            writer.write(line + '<CODESPLIT>' + '<CODESPLIT>'.join([str(l) for l in logit]) + '\\n')\n",
    "        print(\"Accuracy =\", str(accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation {} *****\n",
      "  Num examples = 1000000\n",
      "  Batch size = 32\n",
      "=========================[Step 5000 / 31250] \n",
      "=========================[Step 10000 / 31250] \n",
      "=========================[Step 15000 / 31250] \n",
      "========================[Step 20000 / 31250] \n",
      "=========================[Step 25000 / 31250] \n",
      "=========================[Step 30000 / 31250] \n",
      "======acc = 0.989881\n",
      "acc_and_f1 = 0.4949405\n",
      "f1 = 0.0\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(model, tokenizer, test0_dataset, test0_lines, \"./python_results/batch0_result__v2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import chunked\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python mrr: 0.7634221178177741\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "ranks = []\n",
    "num_batch = 0\n",
    "file = 'python_results/batch0_result__v2.txt'\n",
    "with open(file, encoding='utf-8') as f:\n",
    "    batched_data = chunked(f.readlines(), batch_size)\n",
    "    for batch_idx, batch_data in enumerate(batched_data):\n",
    "        num_batch += 1\n",
    "        step1 = batch_data[batch_idx].strip().split('<CODESPLIT>')\n",
    "        correct_score = float(step1[-1])\n",
    "        scores = np.array([float(data.strip().split('<CODESPLIT>')[-1]) for data in batch_data])\n",
    "        rank = np.sum(scores >= correct_score)\n",
    "        ranks.append(rank)\n",
    "\n",
    "mean_mrr = np.mean(1.0 / np.array(ranks))\n",
    "print(\"Python mrr: {}\".format(mean_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
